---
title: "Principal Component Analysis"
author: "Pushpakanthie Wijekoon"
date: 2018-07-23T21:13:14-05:00
output:
  html_document:
    df_print: paged
---

In some statistical techniques, it is essential to convert a set of correlated variables to a set of uncorrelated variables. This can be done by using Principal Component Analysis (PCA). The converted uncorrelated variables are called principal components that represent most of the information in the original set of variables.  
This statistical technique is also a useful descriptive tool to examine your data, and to reduce the number of variables of the original data set.  

To obtain Principal components we can either use the covariance matrix of the data set or the correlation matrix of the data set. If the variances of the variables are significantly different, we can use the correlation matrix, and use of correlation matrix also implies that we consider standardized variables for the analysis.  
When finding principal components, we try to find linear combinations of a set of variables that maximize the variation contained within them and displaying them using fewer dimensions. 
More details  of this method can be learned from this [ebook](http://cda.psych.uiuc.edu/statistical_learning_course/Jolliffe%20I.%20Principal%20Component%20Analysis%20(2ed.,%20Springer,%202002)(518s)_MVsa_.pdf)   

## Example  
Consider an example in which yields of 8 varieties of oat gathered from two experimental stations located in the north-eastern region of Poland (Dmowski et al. 1989).The oat varieties considered in this study are:   
A- Markus(V1), B - Perona(V2), C - Borek(V3), D - Rumak(V4), E - Dragon(V5), F - Boruta(V6), G - UÃlan(V7), H - PÃlatek(V8).

The locations of the experimental stations were 'Wrocikowo', and 'Garbno'.

Now we read the data set. Suppose your data set is a .csv file saved in your working directory. Using dplyr package you can present this data set in table form in your output document. The rows in this data set correspond to the locations and the columns represents varieties. Now, we read the data, show first 5 lines of the data, and check the variable names using the following codes.   

```{r}
suppressMessages(library(dplyr))
oat_data <- read.csv("oat.csv")
head(oat_data)
names(oat_data)
```

Now without considering the variable 'location', we can compare the yields of 8 varieties using the boxplots as below:  
```{r}
#Select the last 8 columns
oat_data2=oat_data[,2:9]
boxplot(oat_data2)
```
  
Note the differences between the yields of 8 varieties. The varieties C, D, and E contain some outliers.  

Now we find summary statistics, covariances and correlations of 8 varieties. We usually do this to check whether we have to use the covariance or correlation matrix to obtain the principal components. Also, to check whether the data can be reduced, the variables should be correlated.   

```{r}
library(knitr)
kable(summary(oat_data2))
S=cov(oat_data2)
S
#Total variance
sum(diag(S))
cor(oat_data2)

```
  
After obtaining the correlations, Bartlett’s test of sphericity (the null hypothesis of the test is that there is no correlation between the variables) can be used to identify whether significant correlations exists. The test statistic follows a chi square distribution.      
 
```{r}
suppressMessages(library(REdaS))
bart_spher(oat_data2)
```
 
  
According to the Bartlett’s test of sphericity results, there is a significant (p < 0.000) correlation among variables. This indicates that PCA can be performed, and the variables can be grouped.  
Now, the correlations among variables can be visualized as follows:    

```{r}
pairs(oat_data2)
suppressMessages(library(GGally))  
ggpairs(data = oat_data2,columns = 1:8)

```
  
Since the means and variances of the yields of 8 varieties are not much different, we can use the covariance matrix to obtain principal component analysis. However, it is advisable to scale (standardized) the variables if you expect to visualize principal components using a biplot since the vectors representing the varieties should be of unit length.
You can use the functions prcomp() and princomp to compute the principal components in R. The difference between the two is simply the method employed to calculate PCA. To get help from R just type ?princomp in the console.    

```{r}
pr.out <- prcomp(oat_data2, scale=TRUE)  
#loadings of the principal components  
kable(pr.out$rotation)  

```
  
The above values are called loadings of the principal components. The values for  PC1 have the same sign and the values are approximately equal. Therefore, PC1 can be considered as some kind of average of the measurements of all varieties. 
However, PC2 has a negative weight for 'B', 'D', 'F', 'H', and positive weights for other varieties. These groupings indicate the differences and similarities between the the yields of 8 varieties.  

Now we try to calculate the proportion of variance explained by each PC, since it is important to identify the number of principal components that we can retain further.  

```{r}
summary(pr.out)
```
  
Note that the proportion of variance of the first principal component explains 95.69% of the variance of the data, and the contribution of the other principal components is very less. Hence, the first PC is sufficient to explain the variation of the data. This infromation can be shown visually by using the following graph.  

```{r}
pr.var <- pr.out$sdev^2
pve <- pr.var / sum(pr.var)
plot(pve, xlab='Principal Component', 
     ylab='Proportion of Variance Explained', 
     ylim=c(0,1), 
     type='b')
```
  
Check the steepness of the graph.   
In this case the non-steep of the graph can be seen after the second point. This indicates that  one principal component is sufficient to represent data.  

For this data set we noted that the first PC is sufficient to explain the variation of the data based on the proportion of variance of the principal components. There are some other methods to identify the number of principal components. Jolliffe (1972) suggested the cutoff value as 0.7 if one uses correlation matrix for the analysis, and 0.7(average eigenvalue) if one uses covariance matrix for the analysis.   
Cattell (1966) proposed to draw a 'Scree plot'. A scree graph of the eigenvalues can be plotted to visualize the proportion of variance explained by each subsequential eigenvalue.     

```{r}
R.eigen <- eigen(cor(oat_data2))
R.eigen
plot(R.eigen$values, xlab = 'Eigenvalue Number', ylab = 'Size of the Eigenvalue', main = 'Scree Graph')
lines(R.eigen$values)
```
  
According to Jolliffe's method only one eigen value is greater than 0.7. Also check the steepness of the graph.   
In this case the non-steep of the graph can be seen after the second eigen value. This indicates that  one principal component is sufficient to represent data.    

## Visualization of Principal Components  
The first two principal components are often plotted as a scatter plot which may reveal interesting features of the data. To show the spread of the data and principal components, we can draw a biplot. Biplot is a multivariate scatter plot.      

```{r}
#biplot  
biplot(pr.out, scale=0)
```
  
The parameter scale = 0 ensures that arrows are scaled to represent the loadings. Although the loadings of the principal components of the first two PC's indicate two clear groupings as {A, E, C, G} and {B, D, F, H}, further grouping is also visible. So we identify fiver grouping of yield as {B}, {D}, {F, H}, {A,E,C} and {G}. Further, the yields of the varieties F and H are very similar, and C and E are very similar. 
The arrows show how the variables are moving across the two dimensions. We can see that the three varieties B, F and H are moving along the first PC while the other varieties except D moves mostly across the second PC. The locations colored in black show how each location varies across the PC directions. For example, location 11 (Garbno) is mostly associated with the variety B.  
Since the angles between the standardized variables are all acute, it indicates that the variables are positively correlated.   

The ggfortify package provides a nice scatter plot for the first two principal components with autoplot() function.  


```{r}
library(ggfortify)
pca.plot <- autoplot(pr.out, data = oat_data,colour = 'location')
pca.plot
```
  
The points of the two groups are clustered for most of the part. However, the two points at the bottom and left of the graph may be outliers.   


## What are the conclusions?  
One variable to represent yields of all varieties  

  |       PC1|        PC2|        PC3|        PC4|        PC5|        PC6|        PC7|        PC8|
|:--|---------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|
|A  | 0.3586163|  0.1653196|  0.1058518| -0.2486012|  0.4066296| -0.0227407| -0.7056197| -0.3275557|
|B  | 0.3478702| -0.6368095|  0.0545720|  0.3647403|  0.2813617|  0.5059168|  0.0424513|  0.0229796|
|C  | 0.3563794|  0.2848707|  0.0601137|  0.5252810| -0.3239142| -0.0781449|  0.1708394| -0.6099964|
|D  | 0.3519441| -0.0483905|  0.5967907| -0.4777262| -0.4690630|  0.2250698|  0.1099749|  0.0814948|
|E  | 0.3558807|  0.2446484|  0.3314327|  0.3693313|  0.2109387| -0.3845326|  0.0075329|  0.6122301|
|F  | 0.3566816| -0.2342416| -0.1696205| -0.3765736|  0.3246585| -0.4661009|  0.5313093| -0.2058825|
|G  | 0.3486915|  0.5236906| -0.4756094| -0.1509746|  0.0861847|  0.5037623|  0.2127402|  0.2206854|
|H  | 0.3522090| -0.3076952| -0.5110983| -0.0036162| -0.5227059| -0.2605306| -0.3625099|  0.2180065|

Yields of 8 varieties are in five different groups
Variances of the yields are mostly similar  

Further details:   
1. https://rpubs.com/Buczman/PCAPokemon  
2. http://www.rpubs.com/aaronsc32/principal-component-analysis      
3. https://rpubs.com/ryankelly/pca   
4. https://rpubs.com/JanpuHou/278584  

Reading data in different formats:
http://www.sthda.com/english/wiki/reading-data-from-txt-csv-files-r-base-functions  

